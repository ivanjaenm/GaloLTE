# GaloLTE
Galore (low-rank gradients) + LTE (low-rank weights) = GaloLTE ftw!

References:

Galore paper: https://arxiv.org/abs/2403.03507
"GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection"
by
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, Yuandong Tian

LTE paper: https://minyoungg.github.io/LTE/
"Training Neural Networks From Scratch with Parallel Low-Rank Adapters"
by
Minyoung Huh, Brian Cheung, Jeremy Bernstein, Phillip Isola, Pulkit Agrawal
